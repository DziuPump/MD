# LLM-Based Test Case Generation and Evaluation

This repository contains research data and examples used in my master's thesis, which explores how Large Language Models (LLMs) can generate and evaluate software test cases from system requirements.

# Data Directory

XY example contains:
- **X** → The *input*: a system requirement (non-functional)
- **y** → The *output*: test cases and evaluation metrics generated by multiple LLMs

### Example structure

```json
{
  "X": {
    "id": "NFR-205",
    "title": "API response time under load",
    "description": "The system shall respond to API requests within 2 seconds under 500 concurrent users."
  },
  "y": {
    "models": [
      {
        "name": "GPT-4o-mini",
        "coverage_score": 0.92,
        "testability_score": 0.95,
        "quality_score": 0.93
      },
      {
        "name": "Gemini-1.5-pro",
        "coverage_score": 0.96,
        "testability_score": 0.89,
        "quality_score": 0.91
      }
    ],
    "comparison_summary": {
      "best_coverage": "Gemini-1.5-pro",
      "best_testability": "GPT-4o-mini"
    }
  }
}
Dataset (/data/Filtered_Non-Functional_Requirements.csv) contains

```

Dataset (Filtered_Non-Functional_Requirements.csv) contains non-functional requirements examples
