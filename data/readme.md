# LLM-Based Test Case Generation and Evaluation

This repository contains research data and examples used in my master's thesis, which explores how Large Language Models (LLMs) can generate and evaluate software test cases from system requirements.

Non-functional requirements are transformed into usable test cases using various tools, including LLMs, Microsoft Azure, Jira. Generated Test cases coverage of corresponding requirement will be evaluated and compared between various LLMs.

# Data Directory

XY example contains:
- **X** → The *input*: a system requirement (non-functional)
- **y** → The *output*: test cases, evaluation metrics (such as coverage, testability, clarity, consistency and overall quality), non-functional requirement's classification generated by multiple LLMs

### Example structure

```json
{
  "X": {
    "id": "NFR-205",
    "title": "API response time under load",
    "description": "The system shall respond to API requests within 2 seconds under 500 concurrent users."
  },
  "y": {
    "models": [
      {
        "name": "GPT-4o-mini",
        "coverage_score": 0.92,
        "testability_score": 0.95,
        "quality_score": 0.93
      },
      {
        "name": "Gemini-1.5-pro",
        "coverage_score": 0.96,
        "testability_score": 0.89,
        "quality_score": 0.91
      }
    ],
    "comparison_summary": {
      "best_coverage": "Gemini-1.5-pro",
      "best_testability": "GPT-4o-mini"
    }
  }
}
```

Dataset (Filtered_Non-Functional_Requirements.csv) contains non-functional requirements examples

### Planned extensions:

Integrate additional LLMs (e.g., Mistral, Llama 3).

Automate metric calculation and visualization.

Develop a custom web dashboard replacing Jira for managing requirements and generated test cases.
